{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ec9905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fa5dafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_excel(\"data/online_retail.xlsx\")\n",
    "# df.to_csv(\"data/online_retail.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "999fb412",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[2]\") \\\n",
    "      .appName(\"SparkFirst\")   \\\n",
    "      .config(\"spark.executor.memory\", \"32g\")\\\n",
    "      .config(\"spark.executor.cores\", 4) \\\n",
    "      .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "      .config(\"spark.dynamicAllocation.maxExecutors\", 4) \\\n",
    "      .config(\"spark.shuffle.service.enabled\", \"true\") \\\n",
    "      .getOrCreate()\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", \"true\").csv(\"data/online_retail.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2976e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество строк в файле: 541909\n",
      "Количество уникальных клиентов: 4372\n",
      "В какой стране совершается большинство покупок: United Kingdom\n",
      "Даты самой ранней и самой последней покупки на платформе: 2010-12-01 и 2011-12-09\n"
     ]
    }
   ],
   "source": [
    "print(\"Количество строк в файле: {}\".format(df.count()))\n",
    "print(\"Количество уникальных клиентов: {}\".format(\n",
    "  df.select(countDistinct(\"CustomerID\")).collect()[0][0]\n",
    "))\n",
    "df.createOrReplaceTempView(\"df_t\")\n",
    "print(\"В какой стране совершается большинство покупок: {}\".format(\n",
    "  spark.sql(\"select count (*) as cnt, Country from df_t group by Country order by cnt desc limit 1\").collect()[0][1]\n",
    "))\n",
    "dates = spark.sql(\"select min(InvoiceDate), max(InvoiceDate) from df_t\").collect()[0]\n",
    "print(\"Даты самой ранней и самой последней покупки на платформе: {} и {}\".format(dates[0].date(), dates[1].date()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eadd6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---------+------------------+\n",
      "|CustomerID|recency|frequency|          monetary|\n",
      "+----------+-------+---------+------------------+\n",
      "|   16916.0|     46|      143|4.0297902097902085|\n",
      "|   17884.0|     26|      117| 6.132051282051281|\n",
      "|   13094.0|     44|       30| 56.96200000000002|\n",
      "|   16596.0|     38|       12|20.845833333333335|\n",
      "|   17633.0|     54|       72|17.254722222222217|\n",
      "|   18114.0|    313|       28| 7.860714285714286|\n",
      "|   13973.0|    310|       11|24.063636363636363|\n",
      "|   14473.0|     97|        7| 33.47714285714286|\n",
      "|   13956.0|     28|      152| 6.752763157894736|\n",
      "|   13533.0|    205|       76|3.5630263157894753|\n",
      "|   13918.0|     72|       30|            40.428|\n",
      "|   12493.0|    188|       23|18.121304347826086|\n",
      "|   14285.0|     44|       27| 70.74111111111112|\n",
      "|   15776.0|    156|       18|13.423333333333336|\n",
      "|   14768.0|     40|        6|23.250000000000004|\n",
      "|   17267.0|    150|       38| 8.358421052631577|\n",
      "|   14024.0|    144|       16|          20.48125|\n",
      "|   16858.0|    389|       13|26.606923076923085|\n",
      "|   13649.0|    279|       23|15.069565217391307|\n",
      "|   16656.0|     45|       80|107.10300000000002|\n",
      "+----------+-------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Предполагаем, что анализ происходит 2012-01-01\n",
    "# Добавляем столбцы RFM\n",
    "df = spark.sql(\n",
    "    \"\"\" select CustomerID,\n",
    "        min(datediff(date '2012-01-01', InvoiceDate)) as recency, \n",
    "        count(*) as frequency,\n",
    "        avg(Quantity * UnitPrice) as monetary\n",
    "        from df_t \n",
    "        where CustomerID is not null\n",
    "        group by CustomerID\"\"\" \n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8eeac221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---------+------------------+--------+--------+--------+------+\n",
      "|CustomerID|recency|frequency|          monetary|r_rating|f_rating|m_rating|rating|\n",
      "+----------+-------+---------+------------------+--------+--------+--------+------+\n",
      "|   16916.0|     46|      143|4.0297902097902085|       B|       A|       C|   BAC|\n",
      "|   17884.0|     26|      117| 6.132051282051281|       A|       A|       C|   AAC|\n",
      "|   13094.0|     44|       30| 56.96200000000002|       B|       B|       A|   BBA|\n",
      "|   16596.0|     38|       12|20.845833333333335|       B|       C|       C|   BCC|\n",
      "|   17633.0|     54|       72|17.254722222222217|       B|       A|       C|   BAC|\n",
      "|   18114.0|    313|       28| 7.860714285714286|       C|       B|       C|   CBC|\n",
      "|   13973.0|    310|       11|24.063636363636363|       C|       C|       C|   CCC|\n",
      "|   14473.0|     97|        7| 33.47714285714286|       C|       C|       B|   CCB|\n",
      "|   13956.0|     28|      152| 6.752763157894736|       A|       A|       C|   AAC|\n",
      "|   13533.0|    205|       76|3.5630263157894753|       C|       A|       C|   CAC|\n",
      "|   13918.0|     72|       30|            40.428|       C|       B|       B|   CBB|\n",
      "|   12493.0|    188|       23|18.121304347826086|       C|       B|       C|   CBC|\n",
      "|   14285.0|     44|       27| 70.74111111111112|       B|       B|       A|   BBA|\n",
      "|   15776.0|    156|       18|13.423333333333336|       C|       C|       C|   CCC|\n",
      "|   14768.0|     40|        6|23.250000000000004|       B|       C|       C|   BCC|\n",
      "|   17267.0|    150|       38| 8.358421052631577|       C|       B|       C|   CBC|\n",
      "|   14024.0|    144|       16|          20.48125|       C|       C|       C|   CCC|\n",
      "|   16858.0|    389|       13|26.606923076923085|       C|       C|       B|   CCB|\n",
      "|   13649.0|    279|       23|15.069565217391307|       C|       B|       C|   CBC|\n",
      "|   16656.0|     45|       80|107.10300000000002|       B|       A|       A|   BAA|\n",
      "+----------+-------+---------+------------------+--------+--------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Добавляем столбцы с рейтингами\n",
    "df.createOrReplaceTempView(\"df_t\")\n",
    "df = spark.sql(\n",
    "    \"\"\" select CustomerID, recency, frequency, monetary,\n",
    "        case\n",
    "          when recency < 30 then 'A'\n",
    "          when recency < 60 then 'B'\n",
    "          else 'C'\n",
    "        end as r_rating,\n",
    "        case\n",
    "          when frequency > 50 then 'A'\n",
    "          when frequency > 20 then 'B'\n",
    "          else 'C'\n",
    "        end as f_rating,\n",
    "        case\n",
    "          when monetary > 50 then 'A'\n",
    "          when monetary > 25 then 'B'\n",
    "          else 'C'\n",
    "        end as m_rating,\n",
    "        concat(r_rating, f_rating, m_rating) as rating\n",
    "        from df_t\"\"\"\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69e4fe19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|CustomerID|\n",
      "+----------+\n",
      "|   16353.0|\n",
      "|   18092.0|\n",
      "|   17949.0|\n",
      "|   15061.0|\n",
      "|   12753.0|\n",
      "|   13408.0|\n",
      "|   13694.0|\n",
      "|   15187.0|\n",
      "|   14739.0|\n",
      "|   16525.0|\n",
      "|   17677.0|\n",
      "|   16672.0|\n",
      "|   16210.0|\n",
      "|   18102.0|\n",
      "|   13199.0|\n",
      "|   13881.0|\n",
      "|   17857.0|\n",
      "|   17511.0|\n",
      "|   13777.0|\n",
      "|   15694.0|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"df_t\")\n",
    "df = spark.sql(\n",
    "    \"\"\" select CustomerID\n",
    "        from df_t\n",
    "        where rating = 'AAA'\"\"\"\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e433c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "303938a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.coalesce(1).write.mode(\"overwrite\").csv(\"result\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
